---
title: 'STAT847 Assignment 1'
author: "Anh Pham - ewa589"
date: 'September 2023'
output:
  html_document:
    theme: united
    toc: yes
    toc_float: no
    number_sections: no
    highlight: tango
    fig_width: 10
    fig_height: 8
  word_document:
    toc: yes
  pdf_document:
    toc: yes
    fig_height: 6
    fig_width: 10
    number_sections: no
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1
This question involves the use of multiple linear regression on the Auto data set.
```{r}
data(Auto, package = "ISLR")
```
(a) Produce a scatterplot matrix which includes all of the variables in the data set.
```{r}
pairs(Auto)
```
(b) Compute the matrix of correlations between the variables using the function `cor()`. You will need to exclude the name variable, which is qualitative.
```{r}
cor(subset(Auto, select = -name))
```
(c) Use the `lm()` function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the `summary()` function to print the results.
```{r}
lm.fit1 <-  lm(mpg ~ . - name, data = Auto)
summary(lm.fit1)
```
Comment on the output. For instance:

i. Is there a relationship between the predictors and the response?

Yes, there is a relationship between the predictors and the response by testing the null hypothesis of whether all the regression coefficients are zero. The F-statistic is far from 1 (with a small p-value), indicating evidence against the null hypothesis.

ii. Which predictors appear to have a statistically significant relationship to the response?

Looking at the p-values associated with each predictor’s t-statistic, we see that `displacement`, `weight`, `year`, and `origin` have low p-values less than the assumed significance level of a = 0.05, which mean they have a statistically significant relationship to the response, while `cylinders`, `horsepower`, and `acceleration` do not.

iii. What does the coefficient for the `year` variable suggest?

The regression coefficient for `year`, 0.7507727, suggests that for every unit increase of year, `mpg` increases by the coefficient assuming all other predictors are held constant. In other words, cars become more fuel efficient every year by 0.7507727 `mpg` per `year`.

(d) Use the `plot()` function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?
```{r}
par(mfrow = c(2, 2))
plot(lm.fit1)
```
Seems to be non-linear pattern, linear model not the best fit. From the leverage plot, point 14 appears to have high leverage, although not a high magnitude residual.
```{r}
plot(predict(lm.fit1), rstudent(lm.fit1))
```
There are possible outliers as seen in the plot of studentized residuals because there are data with a value greater than 3.

(e) Use the `*` and `:` symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?
```{r}
lm.fit2 <- lm(mpg~.*., Auto[,-9])
summary(lm.fit2)
```
For those whose p-values less than the assumed significance level of a = 0.05, `acceleration:origin` appear to be statistically significant

(f) (1 mark) Try a few different transformations of the variables, such as $log(X)$,$\sqrt{X}$, $X^2$. Comment on your findings.

$log(X)$ transformation
```{r}
lm.fit3 = lm(mpg~.+I(log(cylinders)) +  I(log(displacement)) + I(log(horsepower)) + I(log(weight)) + I(log(acceleration)) + I(log(year)), Auto[,-9])
summary(lm.fit3)
```
$\sqrt{X}$ transformation
```{r}
lm.fit4 = lm(mpg~.+sqrt(cylinders) +  sqrt(displacement) + sqrt(horsepower) + I(sqrt(weight)) + I(sqrt(acceleration)) + I(sqrt(year)), Auto[,-9])
summary(lm.fit4)
```
$X^2$ transformation
```{r}
lm.fit5 = lm(mpg~.+I(cylinders^2) +  I(displacement^2) + I(horsepower^2) + I(weight^2) + I(acceleration^2) + I(year^2), Auto[,-9])
summary(lm.fit5)
```
# Question 2
In this problem we will investigate the t-statistic for the null hypothesis $H_0$ : $\beta$ = 0 in simple linear regression without an intercept. To begin, we generate a predictor x and a response y as follows.
```{r}
set.seed (1)
x=rnorm (100)
y=2*x+rnorm (100)
```
(a) Perform a simple linear regression of y onto x, without an intercept. Report the coefficient estimate $\hat\beta$, the standard error of this coefficient estimate, and the t-statistic and p-value associated with the null hypothesis $H_0$ : $\beta$ = 0. Comment on these results. (You can perform regression without an intercept using the command `lm(y ∼ x + 0)`.)
```{r}
lm.fit6 <- lm(y ~ x + 0)
summary(lm.fit6)
```
According to the summary above, we have a value of 1.9939 for $\hat\beta$, a value of 0.1065 for the `standard error`, a value of 18.73 for the `t-statistic` and a value less than $2e^{-16}$ for the `p-value`. The small `p-value` allows us to reject $H_0$.

(b) Now perform a simple linear regression of x onto y without an intercept, and report the coefficient estimate, its standard error, and the corresponding t-statistic and p-values associated with the null hypothesis $H_0$ : $\beta$ = 0. Comment on these results.
```{r}
lm.fit7 <- lm(x ~ y + 0)
summary(lm.fit7)
```
According to the summary above, we have a value of 0.39111 for $\hat\beta$, a value of 0.02089 for the `standard error`, a value of 18.73 for the `t-statistic` and a value less than $2e^{-16}$ for the `p-value`. The small `p-value` allows us to reject $H_0$.

(c) What is the relationship between the results obtained in (a) and (b)?
We obtain the same value for the t-statistic and consequently the same value for the corresponding p-value. Both results in (a) and (b) reflect the same line created in (a). In other words, y=2x+$\epsilon$ could also be written x=0.5(y−$\epsilon$).

(d) For the regression of $Y$ onto $X$ without an intercept, the `t statistic` for $H_0: \beta=0$ takes the form $\hat{\beta}/SE(\hat{\beta})$, where $\hat{\beta}$ is given by
$$
\hat{\beta}=\left(\sum_{i=1}^{n} x_{i} y_{i}\right) /\left(\sum_{i^{\prime}=1}^{n} x_{i^{\prime}}^2\right) (2)
$$

$$SE(\hat{\beta})=\sqrt{\frac{\sum_{i=1}^{n}\left(y_{i}-x_{i} \hat{\beta}\right)^2}{(n-1) \left(\sum_{i^{\prime}=1}^{n} x_{i^{\prime}}^2\right)}} (3)$$ 

Show algebraically, and confirm numerically in R, that the t-statistic can be written as
$$
\frac{(\sqrt{n-1}) \sum_{i=1}^{n} x_{i} y_{i}}{\sqrt{\left(\sum_{i=1}^{n} x_{i}^2\right)\left(\sum_{i^{\prime}=1}^{n} y_{i^{\prime}}^2\right)-\left(\sum_{i^{\prime}=1}^{n} x_{i^{\prime}} y_{i^{\prime}}\right)^2}} 
$$
First, we note that

$$
    \frac{1}{\text{SE}(\hat{\beta})} = 
    \sqrt{\frac{(n - 1)\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2}{\sum_{i = 1}^n \left( y_i - x_i\hat{\beta} \right)^2}} =
    \frac{\sqrt{(n - 1)} \sqrt{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2}}{\sqrt{\sum_{i = 1}^n \left( y_i - x_i\hat{\beta} \right)^2}}.
$$

Thus, when we combine with $\hat{\beta}$ to compute the t-statistic, we get

$$
    t = \frac{\hat{\beta}}{\text{SE}(\hat{\beta})} = 
    \frac{\sqrt{(n - 1)} \sqrt{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2} \left( \sum_{i = 1}^n x_iy_i \right)}
    {\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2 \right) \sqrt{\sum_{i = 1}^n \left( y_i - x_i\hat{\beta} \right)^2}}.
$$
Note that we can do some cancelling with the $\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2$ that shows up in both the numerator and the denominator. Additionally, $i$ and $i^{\prime}$ are the same in this case, we can rewrite the t-statistic as:

$$
    t = \frac{\sqrt{(n - 1)} \sum_{i = 1}^n x_iy_i}
    {\sqrt{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2} \sqrt{\sum_{i^{\prime} = 1}^n \left( y_{i^{\prime}} - x_{i^{\prime}}\hat{\beta} \right)^2}}
$$

Next, we can combine the two square roots in the denominator and focus there. To start off, we we note that

$$
    \sum_{i^{\prime} = 1}^n \left( y_{i^{\prime}} - x_{i^{\prime}}\hat{\beta} \right)^2 = 
    \sum_{i^{\prime} = 1}^n \left( y_{i^{\prime}}^2 - 2x_{i^{\prime}}y_{i^{\prime}}\hat{\beta} + x_{i^{\prime}}^2\hat{\beta}^2 \right) = 
    \sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 - 2\hat{\beta}\sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} + \hat{\beta}^2\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2.
$$

Plugging in the formula for $\hat{\beta}$, this then becomes

$$
    \sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 - 2\frac{\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} \right)^2}{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2}
    + \frac{\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} \right)^2}{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2} = 
    \sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 - \frac{\left( \sum_{i^{\prime} = 1}^n x_jy_{i^{\prime}} \right)^2}{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2}.
$$

Multiplying this by $\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2$ gives us

$$
    \left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2 \right) \left( \sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 \right) - \left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} \right)^2.
$$

This completes the simplification of the denominator, giving us the desired formula for the `t-statistic` for $H_0: \beta = 0$.

$$
    t = \frac{\left( \sqrt{n-1} \right) \sum_{i = 1}^n x_iy_i}
    {\sqrt{\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2 \right) \left( \sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 \right) - \left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} \right)^2}}
$$
By numerically in R, we can confirm that the above equation is true
```{r}
sum.xy = sum(x * y)
sum.x2 = sum(x**2)
sum.y2 = sum(y**2)
(sqrt(length(x) - 1)*sum.xy)/sqrt(sum.x2*sum.y2 - sum.xy**2)
```
This value of t equals to the `t-value` of the fitted model created in (a)
```{r}
summary(lm.fit6)$coefficients[, "t value"]
```
(e) Using the results from (d), argue that the t-statistic for the regression of $y$ onto $x$ is the same as the t-statistic for the regression of $x$ onto $y$.

The expression of `t statistic` found in (d) is symetric for $x$ and $y$. Thus, when performing regression of $x$ onto $y$, the variables $x$ and $y$ will just replace each other, and the value is not change. This explains why we have the same `t statistic` value in (a) and (b).

(f) In R, show that when regression is performed with an intercept, the t-statistic for $H_0: \beta=0$ is the same for the regression of $y$ onto $x$ as it is for the regression of $x$ onto $y$.

The regression of $y$ onto $x$ with an intercept
```{r}
lm.fit8 <- lm(y ~ x)
summary(lm.fit8)
```
The regression of $x$ onto $y$ with an intercept
```{r}
lm.fit9 <- lm(x ~ y)
summary(lm.fit9)
```
The below results show that `t statistic` values in both regressions are equal
```{r}
summary(lm.fit8)$coefficients[, "t value"]
summary(lm.fit9)$coefficients[, "t value"]
```
# Question 3
In this exercise you will create some simulated data and will fit simple linear regression models to it. Make sure to use set.seed(1) prior to starting part (a) to ensure consistent results.
```{r}
set.seed(1)
```
(a) Using the `rnorm()` function, create a vector, $x$, containing 100 observations drawn from a $N(0,1)$ distribution. This represents a feature, $X$.
```{r}
x = rnorm(100, mean = 0, sd = 1)
```
Note that the conventional notation for a normal distribution with mean $\mu$ and standard deviation $\sigma$ is N($\mu$,$\sigma^2$). In R, the `rnorm()` function takes the arguments n, mean, and sd, where n is the number of observations to be generated, mean is the mean $\mu$ for the normal distribution, and sd is the standard deviation $\sigma$ for the normal distribution. Thus, to generate 100 observations from an N($\mu$=0,$\sigma^2$=0.25) distribution, we need to use the syntax rnorm(100, mean = 0, sd = 0.5).
```{r}
eps = rnorm(100, mean = 0, sd = 0.5)
```
(c) Using $x$ and $\epsilon$, generate a vector $y$ according to the model
$$
Y=-1+0.5 X+\epsilon
$$
```{r}
y = -1 + 0.5*x + eps
```
What is the length of the vector $y$ ? 
```{r}
length(y)
```
What are the values of $\beta_{0}$ and $\beta_{1}$ in this linear model?

In this model, $\beta_{0} = -1$ and $\beta_{1} = 0.5$.

d) Create a scatterplot displaying the relationship between $x$ and $y$. Comment on what you observe.
```{r}
plot(x, y, main = "Plot for original data (variance = .25)")
```
While there is definitely a positive linear relationship between x and y, there is some noise from the introduction of the error term.

e) Fit a least squares linear model to predict $y$ using $x$. Comment on the model obtained. How do $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ compare to $\beta_{0}$ and $\beta_{1}$ ?
```{r}
lm.fit10 <- lm(y~x)
summary(lm.fit10)
```
The least squares regression model to predict $y$ using $x$ has the form $\hat{Y}=-1.01885+0.49947 X$, which is quite close to the true relationship $Y=-1+0.5X+\epsilon$. The `p-values` of essentially zero for $\beta_0$ and $\beta_1$ provide strong evidence to reject the null hypotheses $H_0: \beta_j=0$ for $j=1,2$. We also note, however, that $R^2=0.4674$, which means that the linear regression model explains less than half of the variation in $Y$ found in the data. Lastly, the residual standard error is 0.4814.

(f) Display the least squares line on the scatterplot obtained in (d). Draw the population regression line on the plot, in a different color. Use the legend() command to create an appropriate legend.

```{r}
plot(x, y, main = "Plot for original data (variance = .25)")
abline(lm.fit10, lwd = 2, lty = 2, col = "red")
abline(a = -1, b = 0.5, lwd = 2, lty = 4, col = "blue")
legend(-2.25, 0.4, legend = c("Least squares regression", "Population regression"), col = c("red", "blue"), 
       lty = c(2, 4), lwd = 2, cex = 0.8)
```

(g) Now fit a polynomial regression model that predicts $y$ using $x$ and $x^{2}$. Is there evidence that the quadratic term improves the model fit? Explain your answer.
```{r}
lm.fit11 = lm(y ~ x + I(x^2))
summary(lm.fit11)
```
While the polynomial regression model that predicts $Y$ using $X$ and $X^2$ has a slightly lower residual standard error value (0.479 vs. 0.481) and slightly higher $R^2$ value (0.4779 vs. 0.4674) compared to the linear model using just  $X$, the `p-value` for the coefficient of $X^2$ is 0.164. This means that even though the inclusion of the quadratic term slightly improved the model fit in this situation, there isn't sufficient evidence to reject the null hypothesis $H_0:\beta_2 = 0$. Thus, the improved model fit is likely a result of fitting more closely to the noise in the data from the presence of $\epsilon$ in the true relationship.

(h) Repeat (a)-(f) after modifying the data generation process in such a way that there is less noise in the data. The model in (c) should remain the same. You can do this by decreasing the variance of the normal distribution used to generate the error term $\epsilon$ in (b). Describe your results.

Repeat (a)-(c) with decreased $\epsilon$
```{r}
set.seed(1)
x = rnorm(100, mean = 0, sd = 1)
eps.less.noise = rnorm(100, mean = 0, sd = 0.25)
y.less.noise = -1 + 0.5*x + eps.less.noise
```
Repeat (d) to plot data with less noise
```{r}
plot(x, y.less.noise, main = "Plot for data with less noise (variance = .0625)")
```
Compared to the plot from (d), there is clearly less noise in the data. This results in the points being more tightly clustered around the line $Y=-1+0.5 X+\epsilon$

Repeat (e) to fit a linear model y onto x
```{r}
lm.fit.less.noise = lm(y.less.noise ~ x)
summary(lm.fit.less.noise)
```
Repeat (f) to display the new least square line
```{r}
plot(x, y.less.noise, main = "Plot for data with less noise (variance = .0625)")
abline(lm.fit.less.noise, lwd = 2, lty = 2, col = "red")
abline(a = -1, b = 0.5, lwd = 2, lty = 4, col = "blue")
legend(-2.25, 0.4, legend = c("Least squares regression", "Population regression"), col = c("red", "blue"), 
       lty = c(2, 4), lwd = 2, cex = 0.8)
```
Describe the result

The least squares regression model to predict $y$ using $x$ has the form  $\hat{Y}$=−0.99316+0.50529X, which is again is quite close to the true relationship  $Y=−1+0.5X+\epsilon$. The `p-values` of essentially zero for $\beta_0$ and $\beta_1$ provide strong evidence to reject the null hypotheses $H_0:\beta_j=0$ for j=1,2. We also note, that now $R^2=0.755$, which means that the linear regression model explains about 75% of the variation in Y found in the less noisy data. This is a big improvement from the  $R^2$ value of 0.4674 when using the original, noisier Y. In addition, the residual standard error value of 0.2598 is also an improvement over the value of 0.4814 from the original data.

(i) Repeat (a)-(f) after modifying the data generation process in such a way that there is more noise in the data. The model in (c) should remain the same. You can do this by increasing the variance of the normal distribution used to generate the error term $\epsilon$ in (b). Describe your results.

Repeat (a)-(c) with increased $\epsilon$
```{r}
set.seed(1)
x = rnorm(100, mean = 0, sd = 1)
eps.more.noise = rnorm(100, mean = 0, sd = 1)
y.more.noise = -1 + 0.5*x + eps.more.noise
```
Repeat (d) to plot data with less noise
```{r}
plot(x, y.more.noise, main = "Plot for data with less noise (variance = 1)")
```
Compared to the plot from (d), there is clearly more noise in the data. This results in the points being being very spread out from the line $Y=-1+0.5 X+\epsilon$

Repeat (e) to fit a linear model y onto x
```{r}
lm.fit.more.noise = lm(y.more.noise ~ x)
summary(lm.fit.more.noise)
```
Repeat (f) to display the new least square line
```{r}
plot(x, y.more.noise, main = "Plot for data with less noise (variance = .0625)")
abline(lm.fit.less.noise, lwd = 2, lty = 2, col = "red")
abline(a = -1, b = 0.5, lwd = 2, lty = 4, col = "blue")
legend(-2.25, 0.4, legend = c("Least squares regression", "Population regression"), col = c("red", "blue"), 
       lty = c(2, 4), lwd = 2, cex = 0.8)
```
Describe the result

The least squares regression model to predict $y$ using $x$ has the form  $\hat{Y}$=−0.99316+0.50529X, which is again is quite close to the true relationship  $Y=−1+0.5X+\epsilon$. The `p-values` of essentially zero for $\beta_0$ and $\beta_1$ provide strong evidence to reject the null hypotheses $H_0:\beta_j=0$ for j=1,2. We also note, that now $R^2=0.755$, which means that the linear regression model explains about 75% of the variation in Y found in the less noisy data. This is a big improvement from the  $R^2$ value of 0.4674 when using the original, noisier Y. In addition, the residual standard error value of 0.2598 is also an improvement over the value of 0.4814 from the original data.

(j) What are the confidence intervals for $\beta_{0}$ and $\beta_{1}$ based on the original data set, the noisier data set, and the less noisy data set? Comment on your results. 
```{r}
confint(lm.fit10)
```
```{r}
confint(lm.fit.less.noise)
```

```{r}
confint(lm.fit.more.noise)
```
As we go from less noise from more noise, the confidence intervals for $\beta_0$ and $\beta_1$ become wider and wider.