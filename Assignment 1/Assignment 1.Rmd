---
title: 'STAT847 Assignment 1'
author: "Anh Pham - ewa589"
date: 'September 2023'
output:
  html_document:
    theme: united
    toc: yes
    toc_float: no
    number_sections: no
    highlight: tango
    fig_width: 10
    fig_height: 8
  word_document:
    toc: yes
  pdf_document:
    toc: yes
    fig_height: 6
    fig_width: 10
    number_sections: no
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1
This question involves the use of multiple linear regression on the Auto data set.
```{r}
data(Auto, package = "ISLR")
```
(a) Produce a scatterplot matrix which includes all of the variables in the data set.
```{r}
pairs(Auto)
```
(b) Compute the matrix of correlations between the variables using the function `cor()`. You will need to exclude the name variable, which is qualitative.
```{r}
cor(subset(Auto, select = -name))
```
(c) Use the `lm()` function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the `summary()` function to print the results.
```{r}
lm.fit1 <-  lm(mpg ~ . - name, data = Auto)
summary(lm.fit1)
```
Comment on the output. For instance:

i. Is there a relationship between the predictors and the response?

Yes, there is a relationship between the predictors and the response by testing the null hypothesis of whether all the regression coefficients are zero. The F-statistic is far from 1 (with a small p-value), indicating evidence against the null hypothesis.

ii. Which predictors appear to have a statistically significant relationship to the response?

Looking at the p-values associated with each predictor’s t-statistic, we see that `displacement`, `weight`, `year`, and `origin` have low p-values less than the assumed significance level of a = 0.05, which mean they have a statistically significant relationship to the response, while `cylinders`, `horsepower`, and `acceleration` do not.

iii. What does the coefficient for the `year` variable suggest?

The regression coefficient for `year`, 0.7507727, suggests that for every unit increase of year, `mpg` increases by the coefficient assuming all other predictors are held constant. In other words, cars become more fuel efficient every year by 0.7507727 `mpg` per `year`.

(d) Use the `plot()` function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?
```{r}
par(mfrow = c(2, 2))
plot(lm.fit1)
```
Seems to be non-linear pattern, linear model not the best fit. From the leverage plot, point 14 appears to have high leverage, although not a high magnitude residual.
```{r}
plot(predict(lm.fit1), rstudent(lm.fit1))
```
There are possible outliers as seen in the plot of studentized residuals because there are data with a value greater than 3.

(e) Use the `*` and `:` symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?
```{r}
lm.fit2 <- lm(mpg~.*., Auto[,-9])
summary(lm.fit2)
```
For those whose p-values less than the assumed significance level of a = 0.05, `acceleration:origin` appear to be statistically significant

(f) (1 mark) Try a few different transformations of the variables, such as $log(X)$,$\sqrt{X}$, $X^2$. Comment on your findings.

$log(X)$ transformation
```{r}
lm.fit3 = lm(mpg~.+I(log(cylinders)) +  I(log(displacement)) + I(log(horsepower)) + I(log(weight)) + I(log(acceleration)) + I(log(year)), Auto[,-9])
summary(lm.fit3)
```
$\sqrt{X}$ transformation
```{r}
lm.fit4 = lm(mpg~.+sqrt(cylinders) +  sqrt(displacement) + sqrt(horsepower) + I(sqrt(weight)) + I(sqrt(acceleration)) + I(sqrt(year)), Auto[,-9])
summary(lm.fit4)
```
$X^2$ transformation
```{r}
lm.fit5 = lm(mpg~.+I(cylinders^2) +  I(displacement^2) + I(horsepower^2) + I(weight^2) + I(acceleration^2) + I(year^2), Auto[,-9])
summary(lm.fit5)
```
# Question 2
In this problem we will investigate the t-statistic for the null hypothesis $H_0$ : $\beta$ = 0 in simple linear regression without an intercept. To begin, we generate a predictor x and a response y as follows.
```{r}
set.seed (1)
x=rnorm (100)
y=2*x+rnorm (100)
```
(a) Perform a simple linear regression of y onto x, without an intercept. Report the coefficient estimate $\hat\beta$, the standard error of this coefficient estimate, and the t-statistic and p-value associated with the null hypothesis $H_0$ : $\beta$ = 0. Comment on these results. (You can perform regression without an intercept using the command `lm(y ∼ x + 0)`.)
```{r}
lm.fit6 <- lm(y ~ x + 0)
summary(lm.fit6)
```
According to the summary above, we have a value of 1.9939 for $\hat\beta$, a value of 0.1065 for the `standard error`, a value of 18.73 for the `t-statistic` and a value less than $2e^{-16}$ for the `p-value`. The small `p-value` allows us to reject $H_0$.

(b) Now perform a simple linear regression of x onto y without an intercept, and report the coefficient estimate, its standard error, and the corresponding t-statistic and p-values associated with the null hypothesis $H_0$ : $\beta$ = 0. Comment on these results.
```{r}
lm.fit7 <- lm(x ~ y + 0)
summary(lm.fit7)
```
According to the summary above, we have a value of 0.39111 for $\hat\beta$, a value of 0.02089 for the `standard error`, a value of 18.73 for the `t-statistic` and a value less than $2e^{-16}$ for the `p-value`. The small `p-value` allows us to reject $H_0$.

(c) What is the relationship between the results obtained in (a) and (b)?
We obtain the same value for the t-statistic and consequently the same value for the corresponding p-value. Both results in (a) and (b) reflect the same line created in (a). In other words, y=2x+$\epsilon$ could also be written x=0.5(y−$\epsilon$).

(d) For the regression of $Y$ onto $X$ without an intercept, the `t statistic` for $H_0: \beta=0$ takes the form $\hat{\beta}/SE(\hat{\beta})$, where $\hat{\beta}$ is given by
$$
\hat{\beta}=\left(\sum_{i=1}^{n} x_{i} y_{i}\right) /\left(\sum_{i^{\prime}=1}^{n} x_{i^{\prime}}^2\right) (2)
$$

$$SE(\hat{\beta})=\sqrt{\frac{\sum_{i=1}^{n}\left(y_{i}-x_{i} \hat{\beta}\right)^2}{(n-1) \left(\sum_{i^{\prime}=1}^{n} x_{i^{\prime}}^2\right)}} (3)$$ 

Show algebraically, and confirm numerically in R, that the t-statistic can be written as
$$
\frac{(\sqrt{n-1}) \sum_{i=1}^{n} x_{i} y_{i}}{\sqrt{\left(\sum_{i=1}^{n} x_{i}^2\right)\left(\sum_{i^{\prime}=1}^{n} y_{i^{\prime}}^2\right)-\left(\sum_{i^{\prime}=1}^{n} x_{i^{\prime}} y_{i^{\prime}}\right)^2}} 
$$
First, we note that

$$
    \frac{1}{\text{SE}(\hat{\beta})} = 
    \sqrt{\frac{(n - 1)\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2}{\sum_{i = 1}^n \left( y_i - x_i\hat{\beta} \right)^2}} =
    \frac{\sqrt{(n - 1)} \sqrt{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2}}{\sqrt{\sum_{i = 1}^n \left( y_i - x_i\hat{\beta} \right)^2}}.
$$

Thus, when we combine with $\hat{\beta}$ to compute the t-statistic, we get

$$
    t = \frac{\hat{\beta}}{\text{SE}(\hat{\beta})} = 
    \frac{\sqrt{(n - 1)} \sqrt{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2} \left( \sum_{i = 1}^n x_iy_i \right)}
    {\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2 \right) \sqrt{\sum_{i = 1}^n \left( y_i - x_i\hat{\beta} \right)^2}}.
$$
Note that we can do some cancelling with the $\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2$ that shows up in both the numerator and the denominator. Additionally, $i$ and $i^{\prime}$ are the same in this case, we can rewrite the t-statistic as:

$$
    t = \frac{\sqrt{(n - 1)} \sum_{i = 1}^n x_iy_i}
    {\sqrt{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2} \sqrt{\sum_{i^{\prime} = 1}^n \left( y_{i^{\prime}} - x_{i^{\prime}}\hat{\beta} \right)^2}}
$$

Next, we can combine the two square roots in the denominator and focus there. To start off, we we note that

$$
    \sum_{i^{\prime} = 1}^n \left( y_{i^{\prime}} - x_{i^{\prime}}\hat{\beta} \right)^2 = 
    \sum_{i^{\prime} = 1}^n \left( y_{i^{\prime}}^2 - 2x_{i^{\prime}}y_{i^{\prime}}\hat{\beta} + x_{i^{\prime}}^2\hat{\beta}^2 \right) = 
    \sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 - 2\hat{\beta}\sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} + \hat{\beta}^2\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2.
$$

Plugging in the formula for $\hat{\beta}$, this then becomes

$$
    \sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 - 2\frac{\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} \right)^2}{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2}
    + \frac{\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} \right)^2}{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2} = 
    \sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 - \frac{\left( \sum_{i^{\prime} = 1}^n x_jy_{i^{\prime}} \right)^2}{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2}.
$$

Multiplying this by $\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2$ gives us

$$
    \left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2 \right) \left( \sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 \right) - \left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} \right)^2.
$$

This completes the simplification of the denominator, giving us the desired formula for the `t-statistic` for $H_0: \beta = 0$.

$$
    t = \frac{\left( \sqrt{n-1} \right) \sum_{i = 1}^n x_iy_i}
    {\sqrt{\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2 \right) \left( \sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 \right) - \left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} \right)^2}}
$$
By numerically in R, we can confirm that the above equation is true
```{r}
sum.xy = sum(x * y)
sum.x2 = sum(x**2)
sum.y2 = sum(y**2)
(sqrt(length(x) - 1)*sum.xy)/sqrt(sum.x2*sum.y2 - sum.xy**2)
```
This value of t equals to the `t-value` of the fitted model created in (a)
```{r}
summary(lm.fit6)$coefficients[, "t value"]
```
(e) Using the results from (d), argue that the t-statistic for the regression of $y$ onto $x$ is the same as the t-statistic for the regression of $x$ onto $y$.

The expression of `t statistic` found in (d) is symetric for $x$ and $y$. Thus, when performing regression of $x$ onto $y$, the variables $x$ and $y$ will just replace each other, and the value is not change. This explains why we have the same `t statistic` value in (a) and (b).

(f) In R, show that when regression is performed with an intercept, the t-statistic for $H_0: \beta=0$ is the same for the regression of $y$ onto $x$ as it is for the regression of $x$ onto $y$.

The regression of $y$ onto $x$ with an intercept
```{r}
lm.fit8 <- lm(y ~ x)
summary(lm.fit8)
```
The regression of $x$ onto $y$ with an intercept
```{r}
lm.fit9 <- lm(x ~ y)
summary(lm.fit9)
```
The below results show that `t statistic` values in both regressions are equal
```{r}
summary(lm.fit8)$coefficients[, "t value"]
summary(lm.fit9)$coefficients[, "t value"]
```