---
title: 'STAT847 Assignment 1'
author: "Anh Pham - ewa589"
date: 'September 2023'
output:
  html_document:
    theme: united
    toc: yes
    toc_float: no
    number_sections: no
    highlight: tango
    fig_width: 10
    fig_height: 8
  word_document:
    toc: yes
  pdf_document:
    toc: yes
    fig_height: 6
    fig_width: 10
    number_sections: no
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1
**This question involves the use of multiple linear regression on the Auto data set.**
```{r}
data(Auto, package = "ISLR")
head(Auto)
```
**a. Produce a scatterplot matrix which includes all of the variables in the data set.**
```{r}
pairs(Auto)
```
**b. Compute the matrix of correlations between the variables using the function `cor()`. You will need to exclude the name variable, which is qualitative.**
```{r}
cor(subset(Auto, select = -name))
```
**c. Use the `lm()` function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the `summary()` function to print the results.**
```{r}
lm.fit1 <-  lm(mpg ~ . - name, data = Auto)
summary(lm.fit1)
```
**Comment on the output. For instance:**

i.  Is there a relationship between the predictors and the response?

*Yes, by testing the null hypothesis, which posits that all regression coefficients are zero, there is a relationship between the predictors and the response. The regression model has `F-statistic` = 252.4, significantly different from 1, and a `p-value` of < $2.2e-16$, which is extremely small, as a result, the null hypothesis is rejected. This confirms the presence of a relationship between the predictors and the response.*

ii.  Which predictors appear to have a statistically significant relationship to the response?

*Looking at the `p-values` associated with each predictor’s `t-statistic`, we see that `displacement`, `weight`, `year`, and `origin` have low `p-values` less than the assumed significance level of $a = 0.05$, which mean they have a statistically significant relationship to the response, while `cylinders`, `horsepower`, and `acceleration` do not.*

iii.  What does the coefficient for the `year` variable suggest?

*The regression coefficient for `year` is 0.7507727. This positive value suggests that for each year's increase, the `mpg` rises by this coefficient, assuming all other predictors remain constant. Thus, cars are becoming more fuel-efficient annually, improving by 0.7507727 `mpg` each `year`.*

**d. Use the `plot()` function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?**
```{r}
par(mfrow = c(2, 2))
plot(lm.fit1)
```
From the residual plot, some distinctive patterns are noticed in the deviations from the residual = 0 line, showing that maybe there are potential non-linear relationships. Points situated in the upper-right corner appear to be outliers.

From the leverage plot, point 14 exhibits exceptionally high leverage compared to other data points. However, this case lies within Cook's distance. (Cases outside of Cook's distance can influence the regression outcomes significantly, and excluding them would lead to different regression results.) Given these observations, the leverage plot seems to be admissible.

**e. Use the `*` and `:` symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?**

Using the `*` and `:` to identify interactions between pairs that seem to influence one another: `cylinders:horsepower`, `displacement:weight`, `weight:acceleration`:
```{r}
lm.fit2 <- lm(mpg~.+ cylinders*horsepower + displacement*weight + weight:acceleration, Auto[,-9])
summary(lm.fit2)
```
Considering those with p-values below our predetermined significance level of $a = 0.05$, the interactions `cylinders:horsepower` and `displacement:weight` appear to be statistically significant.

**f. Try a few different transformations of the variables, such as $log(X)$,$\sqrt{X}$, $X^2$. Comment on your findings.**

$log(X)$ transformation for the variables: `displacement`, `horsepower`, `acceleration`
```{r}
lm.fit3 = lm(mpg~.+ I(log(displacement)) + I(log(horsepower)) + I(log(acceleration)), Auto[,-9])
summary(lm.fit3)
```
The bigger $R^2$ (0.863 vs 0.8215 for original data) and smaller residual standard error (2.926 vs 3.328 for original data) show that the model with log transformation provide a better fit to the data compared to the original model.

The logged displacement and horsepower predictors are statistically significant in this model.

$\sqrt{X}$ transformation for the variables: `cylinders`, `weight`, `displacement`
```{r}
lm.fit4 = lm(mpg~.+I(sqrt(cylinders)) +  I(sqrt(displacement)) + I(sqrt(weight)), Auto[,-9])
summary(lm.fit4)
```
The bigger $R^2$ (0.8586 vs 0.8215 for original data) and smaller residual standard error (2.973 vs 3.328 for original data) show that the model with square root transformation provide a better fit to the data compared to the original model.

The square root of weight and displacement predictors are statistically significant in this model.

$X^2$ transformation for the variables: `weight`, `horsepower`, `cylinders`
```{r}
lm.fit5 = lm(mpg~.+I(cylinders^2) + I(horsepower^2) + I(weight^2), Auto[,-9])
summary(lm.fit5)
```
The bigger $R^2$ (0.8645 vs 0.8215 for original data) and smaller residual standard error (2.91 vs 3.328 for original data) show that the model with squaring transformation provide a better fit to the data compared to the original model.

This squaring transformation model highlighted the statistically significance of squared horsepower and squared weight. This also suggests that there might be a quadratic relationship between mpg and these predictors.

# Question 2

**In this problem we will investigate the t-statistic for the null hypothesis $H_0 : \beta = 0$ in simple linear regression without an intercept. To begin, we generate a predictor x and a response y as follows.**
```{r}
set.seed (1)
x=rnorm (100)
y=2*x+rnorm (100)
```
**a. Perform a simple linear regression of y onto x, without an intercept. Report the coefficient estimate $\hat\beta$, the standard error of this coefficient estimate, and the `t-statistic` and `p-value` associated with the null hypothesis $H_0 : \beta = 0$. Comment on these results. (You can perform regression without an intercept using the command `lm(y ∼ x + 0)`.)**
```{r}
lm.fit6 <- lm(y ~ x + 0)
summary(lm.fit6)
```
According to the summary above, we have a value of 1.9939 for $\hat\beta$, a value of 0.1065 for the `standard error`, a value of 18.73 for the `t-statistic` and a value less than $2e^{-16}$ for the `p-value`. The small `p-value` allows us to reject $H_0$.

**b. Now perform a simple linear regression of x onto y without an intercept, and report the coefficient estimate, its standard error, and the corresponding t-statistic and p-values associated with the null hypothesis $H_0 : \beta = 0$. Comment on these results.**
```{r}
lm.fit7 <- lm(x ~ y + 0)
summary(lm.fit7)
```
According to the summary above, we have a value of 0.39111 for $\hat\beta$, a value of 0.02089 for the `standard error`, a value of 18.73 for the `t-statistic` and a value less than $2e^{-16}$ for the `p-value`. The small `p-value` allows us to reject $H_0$.

**c. What is the relationship between the results obtained in (a) and (b)?**

We obtain the same value for the `t-statistic` and consequently the same value for the corresponding `p-value`. Both results in (a) and (b) reflect the same line created in (a). In other words, y=2x+$\epsilon$ could also be written x=0.5(y−$\epsilon$).

**d. For the regression of $Y$ onto $X$ without an intercept, the `t statistic` for $H_0: \beta=0$ takes the form $\hat{\beta}/SE(\hat{\beta})$, where $\hat{\beta}$ is given by**
$$
\hat{\beta}=\left(\sum_{i=1}^{n} x_{i} y_{i}\right) /\left(\sum_{i^{\prime}=1}^{n} x_{i^{\prime}}^2\right)\; (1)
$$
, and where
$$
SE(\hat{\beta})=\sqrt{\frac{\sum_{i=1}^{n}\left(y_{i}-x_{i} \hat{\beta}\right)^2}{(n-1) \left(\sum_{i^{\prime}=1}^{n} x_{i^{\prime}}^2\right)}}\; (2)
$$ 

Show algebraically, and confirm numerically in R, that the t-statistic can be written as
$$
\frac{(\sqrt{n-1}) \sum_{i=1}^{n} x_{i} y_{i}}{\sqrt{\left(\sum_{i=1}^{n} x_{i}^2\right)\left(\sum_{i^{\prime}=1}^{n} y_{i^{\prime}}^2\right)-\left(\sum_{i^{\prime}=1}^{n} x_{i^{\prime}} y_{i^{\prime}}\right)^2}} 
$$
Starting with the formula for the reciprocal of the standard error of $\hat{\beta}$:

$$
    \frac{1}{\text{SE}(\hat{\beta})} = 
    \sqrt{\frac{(n - 1)\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2}{\sum_{i = 1}^n \left( y_i - x_i\hat{\beta} \right)^2}} =
    \frac{\sqrt{(n - 1)} \sqrt{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2}}{\sqrt{\sum_{i = 1}^n \left( y_i - x_i\hat{\beta} \right)^2}}.
$$

Given the formula for $\hat{\beta}$ in (1), , the `t-statistic` can be expressed as:

$$
    t = \frac{\hat{\beta}}{\text{SE}(\hat{\beta})} = 
    \frac{\sqrt{(n - 1)} \sqrt{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2} \left( \sum_{i = 1}^n x_iy_i \right)}
    {\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2 \right) \sqrt{\sum_{i = 1}^n \left( y_i - x_i\hat{\beta} \right)^2}}.
$$
Note that we can do some cancelling with the $\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2$ that shows up in both the numerator and the denominator. Additionally, $i$ and $i^{\prime}$ indices are the same in this case, we can rewrite the `t-statistic` as:

$$
    t = \frac{\sqrt{(n - 1)} \sum_{i = 1}^n x_iy_i}
    {\sqrt{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2} \sqrt{\sum_{i^{\prime} = 1}^n \left( y_{i^{\prime}} - x_{i^{\prime}}\hat{\beta} \right)^2}}
$$

Next, for the sum of squared residuals, we have:

$$
    \sum_{i^{\prime} = 1}^n \left( y_{i^{\prime}} - x_{i^{\prime}}\hat{\beta} \right)^2 = 
    \sum_{i^{\prime} = 1}^n \left( y_{i^{\prime}}^2 - 2x_{i^{\prime}}y_{i^{\prime}}\hat{\beta} + x_{i^{\prime}}^2\hat{\beta}^2 \right) = 
    \sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 - 2\hat{\beta}\sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} + \hat{\beta}^2\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2.
$$

By substituting the formula for $\hat{\beta}$, this equation then becomes:

$$
    \sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 - 2\frac{\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} \right)^2}{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2}
    + \frac{\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} \right)^2}{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2} = 
    \sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 - \frac{\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} \right)^2}{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2}.
$$

Multiplying this by $\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2$ gives us:

$$
   \left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2 \right)\left(\sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 - \frac{\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} \right)^2}{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2}\right) =\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2 \right) \left( \sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 \right) - \left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} \right)^2.
$$

This completes the simplification of the denominator, giving us the desired formula for the `t-statistic` for $H_0: \beta = 0$.

$$
    t = \frac{\left( \sqrt{n-1} \right) \sum_{i = 1}^n x_iy_i}
    {\sqrt{\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2 \right) \left( \sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 \right) - \left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} \right)^2}}
$$
By numerically in R, we can confirm that the above equation is true
```{r}
sum.xy = sum(x * y)
sum.x2 = sum(x**2)
sum.y2 = sum(y**2)
(sqrt(length(x) - 1)*sum.xy)/sqrt(sum.x2*sum.y2 - sum.xy**2)
```
This value of t equals to the `t-value` of the fitted model created in (a)
```{r}
summary(lm.fit6)$coefficients[, "t value"]
```
**e. Using the results from (d), argue that the t-statistic for the regression of $y$ onto $x$ is the same as the t-statistic for the regression of $x$ onto $y$.**

The expression of `t statistic` found in (d) is symmetric for $x$ and $y$. Thus, when performing regression of $x$ onto $y$, the variables $x$ and $y$ will just replace each other, and the value is not change. This explains why we have the same `t statistic` value in (a) and (b).

**f. In R, show that when regression is performed with an intercept, the t-statistic for $H_0: \beta=0$ is the same for the regression of $y$ onto $x$ as it is for the regression of $x$ onto $y$.**

The regression of $y$ onto $x$ with an intercept
```{r}
lm.fit8 <- lm(y ~ x)
summary(lm.fit8)
```
The regression of $x$ onto $y$ with an intercept
```{r}
lm.fit9 <- lm(x ~ y)
summary(lm.fit9)
```
The below results show that `t statistic` values in both regressions are equal
```{r}
summary(lm.fit8)$coefficients[, "t value"]
summary(lm.fit9)$coefficients[, "t value"]
```
# Question 3
**In this exercise you will create some simulated data and will fit simple linear regression models to it. Make sure to use set.seed(1) prior to starting part (a) to ensure consistent results.**
```{r}
set.seed(1)
```
**a. Using the `rnorm()` function, create a vector, $x$, containing 100 observations drawn from a $N(0,1)$ distribution. This represents a feature, $X$.**
```{r}
x = rnorm(100, mean = 0, sd = 1)
```
**b. Using the `rnorm()` function, create a vector, eps, containing 100 observations drawn from a $\mathrm{N}(0,0.25)$ distribution i.e. a normal distribution with mean zero and variance 0.25 .**

In statistics, the conventional notation for a normal distribution with mean $\mu$ and standard deviation $\sigma$ is N($\mu$,$\sigma^2$). While in R, the `rnorm()` function takes the arguments n, mean, and sd, where n is the number of observations to be generated, mean is the mean $\mu$ for the normal distribution, and sd is the standard deviation $\sigma$ for the normal distribution. Thus, to generate 100 observations from an N($\mu$=0,$\sigma^2$=0.25) distribution, we need to use the syntax `rnorm(100, mean = 0, sd = 0.5)`.
```{r}
eps = rnorm(100, mean = 0, sd = 0.5)
```
**c. Using $x$ and $\epsilon$, generate a vector $y$ according to the model**
$$
Y=-1+0.5 X+\epsilon
$$
```{r}
y = -1 + 0.5*x + eps
```
**What is the length of the vector $y$ ?**
```{r}
length(y)
```
**What are the values of $\beta_{0}$ and $\beta_{1}$ in this linear model?**

In this model, $\beta_{0} = -1$ and $\beta_{1} = 0.5$.

**d. Create a scatterplot displaying the relationship between $x$ and $y$. Comment on what you observe.**
```{r}
plot(x, y, main = "Plot for original data (variance = .25)")
```
While a positive linear correlation between x and y is evident, the introduction of the error term introduces some variability.

**e. Fit a least squares linear model to predict $y$ using $x$. Comment on the model obtained. How do $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ compare to $\beta_{0}$ and $\beta_{1}$ ?**
```{r}
lm.fit10 <- lm(y~x)
summary(lm.fit10)
```
The least squares regression model to predict $y$ using $x$ has the form $\hat{Y}=-1.01885+0.49947 X$, which is quite close to the true relationship $Y=-1+0.5X+\epsilon$. The `p-values` of essentially zero for $\beta_0$ and $\beta_1$ provide strong evidence to reject the null hypotheses $H_0: \beta_j=0$ for $j=1,2$. We also note, however, that $R^2=0.4674$, which means that the linear regression model explains just less than half of the variation in $Y$ found in the data. Lastly, the residual standard error is 0.4814.

**f. Display the least squares line on the scatterplot obtained in (d). Draw the population regression line on the plot, in a different color. Use the legend() command to create an appropriate legend.**

```{r}
plot(x, y, main = "Plot for original data (variance = .25)")
abline(lm.fit10, lwd = 2, lty = 2, col = "red")
abline(a = -1, b = 0.5, lwd = 2, lty = 4, col = "blue")
legend(-2.25, 0.4, legend = c("Least squares regression", "Population regression"), col = c("red", "blue"), 
       lty = c(2, 4), lwd = 2, cex = 0.8)
```

**g. Now fit a polynomial regression model that predicts $y$ using $x$ and $x^{2}$. Is there evidence that the quadratic term improves the model fit? Explain your answer.**
```{r}
lm.fit11 = lm(y ~ x + I(x^2))
summary(lm.fit11)
```
The polynomial regression model, which predicts $Y$ using $X$ and $X^2$, offers a marginally improved fit, with a residual standard error of 0.479 (compared to 0.481 in the linear model) and an $R^2$ value of 0.4779 (versus 0.4674). However, the `p-value` for the $X^2$ coefficient stands at 0.164. This indicates that, despite the slightly better model fit from the quadratic term, there's insufficient statistical evidence to dismiss the null hypothesis $H_0: \beta_2 = 0$. This suggests the improved fit may be due to the model accommodating the data's inherent noise, represented by $\epsilon$, rather than capturing a genuine underlying trend.

**h. Repeat (a)-(f) after modifying the data generation process in such a way that there is less noise in the data. The model in (c) should remain the same. You can do this by decreasing the variance of the normal distribution used to generate the error term $\epsilon$ in (b). Describe your results.**

Repeat (a)-(c) with decreased $\epsilon$
```{r}
set.seed(1)
x = rnorm(100, mean = 0, sd = 1)
eps.less.noise = rnorm(100, mean = 0, sd = 0.25)
y.less.noise = -1 + 0.5*x + eps.less.noise
```
Repeat (d) to plot data with less noise
```{r}
plot(x, y.less.noise, main = "Plot for data with less noise (variance = .0625)")
```
Compared to the plot from (d), there is clearly less noise in the data. This results in the points being more tightly clustered around the line $Y=-1+0.5 X+\epsilon$

Repeat (e) to fit a linear model y onto x
```{r}
lm.fit.less.noise = lm(y.less.noise ~ x)
summary(lm.fit.less.noise)
```
Repeat (f) to display the new least square line
```{r}
plot(x, y.less.noise, main = "Plot for data with less noise (variance = .0625)")
abline(lm.fit.less.noise, lwd = 2, lty = 2, col = "red")
abline(a = -1, b = 0.5, lwd = 2, lty = 4, col = "blue")
legend(-2.25, 0.4, legend = c("Least squares regression", "Population regression"), col = c("red", "blue"), 
       lty = c(2, 4), lwd = 2, cex = 0.8)
```
Describe the result:

The least squares regression model to predict $y$ using $x$ has the form  $\hat{Y}$=−1.00942+0.49973X, which is again is quite close to the true relationship  $Y=−1+0.5X+\epsilon$. The `p-values` of essentially zero for $\beta_0$ and $\beta_1$ provide strong evidence to reject the null hypotheses $H_0:\beta_j=0$ for j=1,2. We also note, that now $R^2=0.7784$, which means that the linear regression model explains about 78% of the variation in Y found in the less noisy data. This is a big improvement from the  $R^2$ value of 0.4674 when using the original, noisier Y. In addition, the residual standard error value of 0.2407 is also an improvement over the value of 0.4814 from the original data.

**i. Repeat (a)-(f) after modifying the data generation process in such a way that there is more noise in the data. The model in (c) should remain the same. You can do this by increasing the variance of the normal distribution used to generate the error term $\epsilon$ in (b). Describe your results.**

Repeat (a)-(c) with increased $\epsilon$
```{r}
set.seed(1)
x = rnorm(100, mean = 0, sd = 1)
eps.more.noise = rnorm(100, mean = 0, sd = 1)
y.more.noise = -1 + 0.5*x + eps.more.noise
```
Repeat (d) to plot data with less noise
```{r}
plot(x, y.more.noise, main = "Plot for data with less noise (variance = 1)")
```
Compared to the plot from (d), there is clearly more noise in the data. This results in the points being being very spread out from the line $Y=-1+0.5 X+\epsilon$

Repeat (e) to fit a linear model y onto x
```{r}
lm.fit.more.noise = lm(y.more.noise ~ x)
summary(lm.fit.more.noise)
```
Repeat (f) to display the new least square line
```{r}
plot(x, y.more.noise, main = "Plot for data with more noise (variance = .0625)")
abline(lm.fit.more.noise, lwd = 2, lty = 2, col = "red")
abline(a = -1, b = 0.5, lwd = 2, lty = 4, col = "blue")
legend(-2.25, 0.4, legend = c("Least squares regression", "Population regression"), col = c("red", "blue"), 
       lty = c(2, 4), lwd = 2, cex = 0.8)
```
Describe the result

The least squares regression model to predict $y$ using $x$ has the form  $\hat{Y}$=−1.03769+0.49894X, which is again is quite close to the true relationship  $Y=−1+0.5X+\epsilon$. The `p-values` of essentially zero for $\beta_0$ and $\beta_1$ provide strong evidence to reject the null hypotheses $H_0:\beta_j=0$ for j=1,2. We also note, that now $R^2=0.1796$, which means that the linear regression model only explains about 18% of the variation in Y found in the less noisy data. This is a big decrease from the  $R^2$ value of 0.4674 when using the original, which is less noisy. In addition, the residual standard error value of 0.9628 is very large in comparison with the value of 0.4814 from the original data.

**j. What are the confidence intervals for $\beta_{0}$ and $\beta_{1}$ based on the original data set, the noisier data set, and the less noisy data set? Comment on your results.**
The confidence intervals for the original data set:
```{r}
confint(lm.fit10)
```
The confidence intervals for the less noisy set:
```{r}
confint(lm.fit.less.noise)
```
The confidence intervals for the noisier data set:
```{r}
confint(lm.fit.more.noise)
```
As the noise level increases, the confidence intervals for $\beta_0$ and $\beta_1$ become wider and wider.

# Question 4
**I collect a set of data ($n=100$ observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. $Y=\beta_{0}+\beta_{1} X+\beta_{2} X_{2}+\beta_{3} X^{3}+\epsilon$**

**a. Suppose that the true relationship between $X$ and $Y$ is linear,i.e. $Y=\beta_{0}+\beta_{1} X+\epsilon$. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.**

**The polynomial regression is expected to have a lower training RSS than the linear regression**, because it could make a tighter fit against data that matched with a wider irreducible error.

**b. Answer (a) using test rather than training RSS.**

For test RSS, the result is reversed compared to (a). Since the true relationship is linear, the cubic regression might overfit the training data, capturing not only the linear trend but also some of the noise. When applied to a test set, this overfitting might penalize the cubic model, leading to a higher test RSS compared to the linear model. **Therefore, the linear model might have a lower test RSS than the cubic model in this scenario.**

**c. Suppose that the true relationship between $X$ and $Y$ is not linear, but we don't know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.**

If the true relationship is not linear, the cubic regression, with its added flexibility, might be better suited to capture the underlying trend in the training data. **Therefore, the training RSS for the polynomial regression model is expected to be lower than that of the linear regression.**

**d. Answer (c) using test rather than training RSS.**

There is not enough information to tell which test RSS would be lower for either
regression, as we do not know “how far it is from linear”. If it is closer to linear than cubic, the linear regression test RSS could be lower than the cubic regression test RSS. Or, if it is closer to cubic than linear, the cubic regression test RSS could be lower than the linear regression test RSS. This is due to bias-variance tradeoff: it is not clear what level of flexibility will fit this data better.

# Question 5
**Consider the fitted values that result from performing linear regression without an intercept. In this setting, the $i$ th fitted value takes the form.**

$$
\hat{y}_{i}=x_{i} \hat{\beta}\;(3),
$$

**where**

$$
\hat{\beta}=\left(\sum_{i=1}^{n} x_{i} y_{i}\right) /\left(\sum_{i^{\prime}=1}^{n} x_{i^{\prime}}^{2}\right)\; (4)
$$
**Show that we can write**
$$
\hat{y}_{i}=\left(\sum_{i^{\prime}=1}^{n} a_{i^{\prime}} y_{i^{\prime}}\right)
$$
**What is $a_{i^{\prime}}$ ?**

**Note: We interpret this result by saying that the fitted values from linear regression are linear combinations of the response values.**


By substituting equation (4) into (3), we have:
$$
\begin{aligned}
\hat{y}_{i}&=x_{i}\hat{\beta}\\
& = x_i\frac{\sum_{i^{\prime}=1}^{n} x_{i^{\prime}} y_{i^{\prime}}} {\sum_{j=1}^{n} x_{j}^{2}}\\
& = \frac{\sum_{i^{\prime}=1}^{n}x_i x_{i^{\prime}}y_{i^{\prime}}}{\sum_{j=1}^{n}x_{j}^{2}}\\
& = \sum_{i^{\prime}=1}^{n}\frac{x_i x_{i^{\prime}}}{\sum_{j=1}^{n}x_{j}^{2}}y_{i^{\prime}}
\end{aligned}
$$
(Note: We've changed the indices used in the formula for $\hat{\beta}$ from $i$ and $i^{\prime}$ to $i^{\prime}$ and $j$ respectively to distinguish them from the $i$ in the $\hat{y}_i$ formula.)

Comparing with the formula for $\hat{y}_{i}=\left(\sum_{i^{\prime}=1}^{n} a_{i^{\prime}} y_{i^{\prime}}\right)$, we find the expression for $a_{i^{\prime}}$ to be: 
$$
a_{i^{\prime}} = \frac{x_i x_{i^{\prime}}}{\sum_{j=1}^{n}x_{j}^{2}}
$$
