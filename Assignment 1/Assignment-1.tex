% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={STAT847 Assignment 1},
  pdfauthor={Anh Pham - ewa589},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{STAT847 Assignment 1}
\author{Anh Pham - ewa589}
\date{September 2023}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{question-1}{%
\section{Question 1}\label{question-1}}

\textbf{This question involves the use of multiple linear regression on
the Auto data set.}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(Auto, }\AttributeTok{package =} \StringTok{"ISLR"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(Auto)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   mpg cylinders displacement horsepower weight acceleration year origin
## 1  18         8          307        130   3504         12.0   70      1
## 2  15         8          350        165   3693         11.5   70      1
## 3  18         8          318        150   3436         11.0   70      1
## 4  16         8          304        150   3433         12.0   70      1
## 5  17         8          302        140   3449         10.5   70      1
## 6  15         8          429        198   4341         10.0   70      1
##                        name
## 1 chevrolet chevelle malibu
## 2         buick skylark 320
## 3        plymouth satellite
## 4             amc rebel sst
## 5               ford torino
## 6          ford galaxie 500
\end{verbatim}

\textbf{a. Produce a scatterplot matrix which includes all of the
variables in the data set.}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pairs}\NormalTok{(Auto)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Assignment-1_files/figure-latex/unnamed-chunk-2-1.pdf}
\textbf{b. Compute the matrix of correlations between the variables
using the function \texttt{cor()}. You will need to exclude the name
variable, which is qualitative.}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(}\FunctionTok{subset}\NormalTok{(Auto, }\AttributeTok{select =} \SpecialCharTok{{-}}\NormalTok{name))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                     mpg  cylinders displacement horsepower     weight
## mpg           1.0000000 -0.7776175   -0.8051269 -0.7784268 -0.8322442
## cylinders    -0.7776175  1.0000000    0.9508233  0.8429834  0.8975273
## displacement -0.8051269  0.9508233    1.0000000  0.8972570  0.9329944
## horsepower   -0.7784268  0.8429834    0.8972570  1.0000000  0.8645377
## weight       -0.8322442  0.8975273    0.9329944  0.8645377  1.0000000
## acceleration  0.4233285 -0.5046834   -0.5438005 -0.6891955 -0.4168392
## year          0.5805410 -0.3456474   -0.3698552 -0.4163615 -0.3091199
## origin        0.5652088 -0.5689316   -0.6145351 -0.4551715 -0.5850054
##              acceleration       year     origin
## mpg             0.4233285  0.5805410  0.5652088
## cylinders      -0.5046834 -0.3456474 -0.5689316
## displacement   -0.5438005 -0.3698552 -0.6145351
## horsepower     -0.6891955 -0.4163615 -0.4551715
## weight         -0.4168392 -0.3091199 -0.5850054
## acceleration    1.0000000  0.2903161  0.2127458
## year            0.2903161  1.0000000  0.1815277
## origin          0.2127458  0.1815277  1.0000000
\end{verbatim}

\textbf{c.~Use the \texttt{lm()} function to perform a multiple linear
regression with mpg as the response and all other variables except name
as the predictors. Use the \texttt{summary()} function to print the
results.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.fit1 }\OtherTok{\textless{}{-}}  \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{{-}}\NormalTok{ name, }\AttributeTok{data =}\NormalTok{ Auto)}
\FunctionTok{summary}\NormalTok{(lm.fit1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = mpg ~ . - name, data = Auto)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.5903 -2.1565 -0.1169  1.8690 13.0604 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  -17.218435   4.644294  -3.707  0.00024 ***
## cylinders     -0.493376   0.323282  -1.526  0.12780    
## displacement   0.019896   0.007515   2.647  0.00844 ** 
## horsepower    -0.016951   0.013787  -1.230  0.21963    
## weight        -0.006474   0.000652  -9.929  < 2e-16 ***
## acceleration   0.080576   0.098845   0.815  0.41548    
## year           0.750773   0.050973  14.729  < 2e-16 ***
## origin         1.426141   0.278136   5.127 4.67e-07 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.328 on 384 degrees of freedom
## Multiple R-squared:  0.8215, Adjusted R-squared:  0.8182 
## F-statistic: 252.4 on 7 and 384 DF,  p-value: < 2.2e-16
\end{verbatim}

\textbf{Comment on the output. For instance:}

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  Is there a relationship between the predictors and the response?
\end{enumerate}

\emph{Yes, by testing the null hypothesis, which posits that all
regression coefficients are zero, there is a relationship between the
predictors and the response. The regression model has
\texttt{F-statistic} = 252.4, significantly different from 1, and a
\texttt{p-value} of \textless{} \(2.2e-16\), which is extremely small,
as a result, the null hypothesis is rejected. This confirms the presence
of a relationship between the predictors and the response.}

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Which predictors appear to have a statistically significant
  relationship to the response?
\end{enumerate}

\emph{Looking at the \texttt{p-values} associated with each predictor's
\texttt{t-statistic}, we see that \texttt{displacement},
\texttt{weight}, \texttt{year}, and \texttt{origin} have low
\texttt{p-values} less than the assumed significance level of
\(a = 0.05\), which mean they have a statistically significant
relationship to the response, while \texttt{cylinders},
\texttt{horsepower}, and \texttt{acceleration} do not.}

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  What does the coefficient for the \texttt{year} variable suggest?
\end{enumerate}

\emph{The regression coefficient for \texttt{year} is 0.7507727. This
positive value suggests that for each year's increase, the \texttt{mpg}
rises by this coefficient, assuming all other predictors remain
constant. Thus, cars are becoming more fuel-efficient annually,
improving by 0.7507727 \texttt{mpg} each \texttt{year}.}

\textbf{d.~Use the \texttt{plot()} function to produce diagnostic plots
of the linear regression fit. Comment on any problems you see with the
fit. Do the residual plots suggest any unusually large outliers? Does
the leverage plot identify any observations with unusually high
leverage?}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(lm.fit1)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Assignment-1_files/figure-latex/unnamed-chunk-5-1.pdf}
From the residual plot, some distinctive patterns are noticed in the
deviations from the residual = 0 line, showing that maybe there are
potential non-linear relationships. Points situated in the upper-right
corner appear to be outliers.

From the leverage plot, point 14 exhibits exceptionally high leverage
compared to other data points. However, this case lies within Cook's
distance. (Cases outside of Cook's distance can influence the regression
outcomes significantly, and excluding them would lead to different
regression results.) Given these observations, the leverage plot seems
to be admissible.

\textbf{e. Use the \texttt{*} and \texttt{:} symbols to fit linear
regression models with interaction effects. Do any interactions appear
to be statistically significant?}

Using the \texttt{*} and \texttt{:} to identify interactions between
pairs that seem to influence one another: \texttt{cylinders:horsepower},
\texttt{displacement:weight}, \texttt{weight:acceleration}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.fit2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg}\SpecialCharTok{\textasciitilde{}}\NormalTok{.}\SpecialCharTok{+}\NormalTok{ cylinders}\SpecialCharTok{*}\NormalTok{horsepower }\SpecialCharTok{+}\NormalTok{ displacement}\SpecialCharTok{*}\NormalTok{weight }\SpecialCharTok{+}\NormalTok{ weight}\SpecialCharTok{:}\NormalTok{acceleration, Auto[,}\SpecialCharTok{{-}}\DecValTok{9}\NormalTok{])}
\FunctionTok{summary}\NormalTok{(lm.fit2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = mpg ~ . + cylinders * horsepower + displacement * 
##     weight + weight:acceleration, data = Auto[, -9])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.4270 -1.5524 -0.1126  1.4025 11.9965 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(>|t|)    
## (Intercept)           4.745e+00  8.108e+00   0.585 0.558724    
## cylinders            -2.591e+00  7.142e-01  -3.628 0.000325 ***
## displacement         -3.556e-02  1.318e-02  -2.699 0.007269 ** 
## horsepower           -2.139e-01  4.439e-02  -4.818 2.09e-06 ***
## weight               -6.099e-03  2.224e-03  -2.742 0.006390 ** 
## acceleration          1.772e-02  2.964e-01   0.060 0.952346    
## year                  7.616e-01  4.510e-02  16.886  < 2e-16 ***
## origin                6.698e-01  2.578e-01   2.598 0.009744 ** 
## cylinders:horsepower  2.530e-02  6.103e-03   4.146 4.18e-05 ***
## displacement:weight   1.053e-05  3.622e-06   2.907 0.003857 ** 
## weight:acceleration  -3.808e-05  1.018e-04  -0.374 0.708710    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.898 on 381 degrees of freedom
## Multiple R-squared:  0.8656, Adjusted R-squared:  0.8621 
## F-statistic: 245.5 on 10 and 381 DF,  p-value: < 2.2e-16
\end{verbatim}

Considering those with p-values below our predetermined significance
level of \(a = 0.05\), the interactions \texttt{cylinders:horsepower}
and \texttt{displacement:weight} appear to be statistically significant.

\textbf{f.~Try a few different transformations of the variables, such as
\(log(X)\),\(\sqrt{X}\), \(X^2\). Comment on your findings.}

\(log(X)\) transformation for the variables: \texttt{displacement},
\texttt{horsepower}, \texttt{acceleration}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.fit3 }\OtherTok{=} \FunctionTok{lm}\NormalTok{(mpg}\SpecialCharTok{\textasciitilde{}}\NormalTok{.}\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(}\FunctionTok{log}\NormalTok{(displacement)) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(}\FunctionTok{log}\NormalTok{(horsepower)) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(}\FunctionTok{log}\NormalTok{(acceleration)), Auto[,}\SpecialCharTok{{-}}\DecValTok{9}\NormalTok{])}
\FunctionTok{summary}\NormalTok{(lm.fit3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = mpg ~ . + I(log(displacement)) + I(log(horsepower)) + 
##     I(log(acceleration)), data = Auto[, -9])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -10.059  -1.499   0.016   1.546  11.800 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(>|t|)    
## (Intercept)           1.031e+02  1.468e+01   7.020 1.02e-11 ***
## cylinders             9.790e-02  2.915e-01   0.336  0.73714    
## displacement          2.377e-02  1.285e-02   1.850  0.06514 .  
## horsepower            1.008e-01  3.223e-02   3.129  0.00189 ** 
## weight               -3.092e-03  6.626e-04  -4.666 4.26e-06 ***
## acceleration          4.406e-01  4.748e-01   0.928  0.35397    
## year                  7.602e-01  4.519e-02  16.821  < 2e-16 ***
## origin                6.019e-01  2.695e-01   2.233  0.02611 *  
## I(log(displacement)) -6.646e+00  2.288e+00  -2.905  0.00388 ** 
## I(log(horsepower))   -1.877e+01  3.611e+00  -5.197 3.32e-07 ***
## I(log(acceleration)) -1.155e+01  7.571e+00  -1.526  0.12793    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.926 on 381 degrees of freedom
## Multiple R-squared:  0.863,  Adjusted R-squared:  0.8594 
## F-statistic: 240.1 on 10 and 381 DF,  p-value: < 2.2e-16
\end{verbatim}

The bigger \(R^2\) (0.863 vs 0.8215 for original data) and smaller
residual standard error (2.926 vs 3.328 for original data) show that the
model with log transformation provide a better fit to the data compared
to the original model.

The logged displacement and horsepower predictors are statistically
significant in this model.

\(\sqrt{X}\) transformation for the variables: \texttt{cylinders},
\texttt{weight}, \texttt{displacement}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.fit4 }\OtherTok{=} \FunctionTok{lm}\NormalTok{(mpg}\SpecialCharTok{\textasciitilde{}}\NormalTok{.}\SpecialCharTok{+}\FunctionTok{I}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(cylinders)) }\SpecialCharTok{+}  \FunctionTok{I}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(displacement)) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(weight)), Auto[,}\SpecialCharTok{{-}}\DecValTok{9}\NormalTok{])}
\FunctionTok{summary}\NormalTok{(lm.fit4)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = mpg ~ . + I(sqrt(cylinders)) + I(sqrt(displacement)) + 
##     I(sqrt(weight)), data = Auto[, -9])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.8718 -1.7364 -0.1811  1.6536 12.0864 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(>|t|)    
## (Intercept)           27.804517  18.006210   1.544  0.12338    
## cylinders             -4.526581   2.834628  -1.597  0.11112    
## displacement           0.106069   0.030304   3.500  0.00052 ***
## horsepower            -0.034356   0.013197  -2.603  0.00959 ** 
## weight                 0.012205   0.004443   2.747  0.00630 ** 
## acceleration          -0.011754   0.091000  -0.129  0.89729    
## year                   0.799408   0.045936  17.403  < 2e-16 ***
## origin                 0.433810   0.275362   1.575  0.11599    
## I(sqrt(cylinders))    21.141131  13.593961   1.555  0.12073    
## I(sqrt(displacement)) -2.993380   0.956069  -3.131  0.00188 ** 
## I(sqrt(weight))       -1.902549   0.511601  -3.719  0.00023 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.973 on 381 degrees of freedom
## Multiple R-squared:  0.8586, Adjusted R-squared:  0.8549 
## F-statistic: 231.3 on 10 and 381 DF,  p-value: < 2.2e-16
\end{verbatim}

The bigger \(R^2\) (0.8586 vs 0.8215 for original data) and smaller
residual standard error (2.973 vs 3.328 for original data) show that the
model with square root transformation provide a better fit to the data
compared to the original model.

The square root of weight and displacement predictors are statistically
significant in this model.

\(X^2\) transformation for the variables: \texttt{weight},
\texttt{horsepower}, \texttt{cylinders}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.fit5 }\OtherTok{=} \FunctionTok{lm}\NormalTok{(mpg}\SpecialCharTok{\textasciitilde{}}\NormalTok{.}\SpecialCharTok{+}\FunctionTok{I}\NormalTok{(cylinders}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(horsepower}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(weight}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{), Auto[,}\SpecialCharTok{{-}}\DecValTok{9}\NormalTok{])}
\FunctionTok{summary}\NormalTok{(lm.fit5)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = mpg ~ . + I(cylinders^2) + I(horsepower^2) + I(weight^2), 
##     data = Auto[, -9])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.2967 -1.5061 -0.1582  1.4841 12.0489 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(>|t|)    
## (Intercept)      8.800e+00  5.290e+00   1.663  0.09705 .  
## cylinders       -1.089e+00  1.260e+00  -0.864  0.38805    
## displacement    -9.569e-04  7.271e-03  -0.132  0.89536    
## horsepower      -2.146e-01  4.043e-02  -5.309 1.88e-07 ***
## weight          -1.252e-02  2.289e-03  -5.471 8.11e-08 ***
## acceleration    -1.807e-01  1.006e-01  -1.795  0.07339 .  
## year             7.682e-01  4.540e-02  16.919  < 2e-16 ***
## origin           7.253e-01  2.533e-01   2.864  0.00442 ** 
## I(cylinders^2)   1.087e-01  1.065e-01   1.020  0.30832    
## I(horsepower^2)  6.321e-04  1.290e-04   4.898 1.43e-06 ***
## I(weight^2)      1.294e-06  3.094e-07   4.181 3.60e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.91 on 381 degrees of freedom
## Multiple R-squared:  0.8645, Adjusted R-squared:  0.861 
## F-statistic: 243.1 on 10 and 381 DF,  p-value: < 2.2e-16
\end{verbatim}

The bigger \(R^2\) (0.8645 vs 0.8215 for original data) and smaller
residual standard error (2.91 vs 3.328 for original data) show that the
model with squaring transformation provide a better fit to the data
compared to the original model.

This squaring transformation model highlighted the statistically
significance of squared horsepower and squared weight. This also
suggests that there might be a quadratic relationship between mpg and
these predictors.

\hypertarget{question-2}{%
\section{Question 2}\label{question-2}}

\textbf{In this problem we will investigate the t-statistic for the null
hypothesis \(H_0 : \beta = 0\) in simple linear regression without an
intercept. To begin, we generate a predictor x and a response y as
follows.}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{ (}\DecValTok{1}\NormalTok{)}
\NormalTok{x}\OtherTok{=}\FunctionTok{rnorm}\NormalTok{ (}\DecValTok{100}\NormalTok{)}
\NormalTok{y}\OtherTok{=}\DecValTok{2}\SpecialCharTok{*}\NormalTok{x}\SpecialCharTok{+}\FunctionTok{rnorm}\NormalTok{ (}\DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{a. Perform a simple linear regression of y onto x, without an
intercept. Report the coefficient estimate \(\hat\beta\), the standard
error of this coefficient estimate, and the \texttt{t-statistic} and
\texttt{p-value} associated with the null hypothesis
\(H_0 : \beta = 0\). Comment on these results. (You can perform
regression without an intercept using the command
\texttt{lm(y\ ∼\ x\ +\ 0)}.)}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.fit6 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+} \DecValTok{0}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(lm.fit6)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = y ~ x + 0)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.9154 -0.6472 -0.1771  0.5056  2.3109 
## 
## Coefficients:
##   Estimate Std. Error t value Pr(>|t|)    
## x   1.9939     0.1065   18.73   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.9586 on 99 degrees of freedom
## Multiple R-squared:  0.7798, Adjusted R-squared:  0.7776 
## F-statistic: 350.7 on 1 and 99 DF,  p-value: < 2.2e-16
\end{verbatim}

According to the summary above, we have a value of 1.9939 for
\(\hat\beta\), a value of 0.1065 for the \texttt{standard\ error}, a
value of 18.73 for the \texttt{t-statistic} and a value less than
\(2e^{-16}\) for the \texttt{p-value}. The small \texttt{p-value} allows
us to reject \(H_0\).

\textbf{b. Now perform a simple linear regression of x onto y without an
intercept, and report the coefficient estimate, its standard error, and
the corresponding t-statistic and p-values associated with the null
hypothesis \(H_0 : \beta = 0\). Comment on these results.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.fit7 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ y }\SpecialCharTok{+} \DecValTok{0}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(lm.fit7)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = x ~ y + 0)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.8699 -0.2368  0.1030  0.2858  0.8938 
## 
## Coefficients:
##   Estimate Std. Error t value Pr(>|t|)    
## y  0.39111    0.02089   18.73   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.4246 on 99 degrees of freedom
## Multiple R-squared:  0.7798, Adjusted R-squared:  0.7776 
## F-statistic: 350.7 on 1 and 99 DF,  p-value: < 2.2e-16
\end{verbatim}

According to the summary above, we have a value of 0.39111 for
\(\hat\beta\), a value of 0.02089 for the \texttt{standard\ error}, a
value of 18.73 for the \texttt{t-statistic} and a value less than
\(2e^{-16}\) for the \texttt{p-value}. The small \texttt{p-value} allows
us to reject \(H_0\).

\textbf{c.~What is the relationship between the results obtained in (a)
and (b)?}

We obtain the same value for the \texttt{t-statistic} and consequently
the same value for the corresponding \texttt{p-value}. Both results in
(a) and (b) reflect the same line created in (a). In other words,
y=2x+\(\epsilon\) could also be written x=0.5(y−\(\epsilon\)).

\textbf{d.~For the regression of \(Y\) onto \(X\) without an intercept,
the \texttt{t\ statistic} for \(H_0: \beta=0\) takes the form
\(\hat{\beta}/SE(\hat{\beta})\), where \(\hat{\beta}\) is given by} \[
\hat{\beta}=\left(\sum_{i=1}^{n} x_{i} y_{i}\right) /\left(\sum_{i^{\prime}=1}^{n} x_{i^{\prime}}^2\right)\; (1)
\] , and where \[
SE(\hat{\beta})=\sqrt{\frac{\sum_{i=1}^{n}\left(y_{i}-x_{i} \hat{\beta}\right)^2}{(n-1) \left(\sum_{i^{\prime}=1}^{n} x_{i^{\prime}}^2\right)}}\; (2)
\]

Show algebraically, and confirm numerically in R, that the t-statistic
can be written as \[
\frac{(\sqrt{n-1}) \sum_{i=1}^{n} x_{i} y_{i}}{\sqrt{\left(\sum_{i=1}^{n} x_{i}^2\right)\left(\sum_{i^{\prime}=1}^{n} y_{i^{\prime}}^2\right)-\left(\sum_{i^{\prime}=1}^{n} x_{i^{\prime}} y_{i^{\prime}}\right)^2}} 
\] Starting with the formula for the reciprocal of the standard error of
\(\hat{\beta}\):

\[
    \frac{1}{\text{SE}(\hat{\beta})} = 
    \sqrt{\frac{(n - 1)\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2}{\sum_{i = 1}^n \left( y_i - x_i\hat{\beta} \right)^2}} =
    \frac{\sqrt{(n - 1)} \sqrt{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2}}{\sqrt{\sum_{i = 1}^n \left( y_i - x_i\hat{\beta} \right)^2}}.
\]

Given the formula for \(\hat{\beta}\) in (1), , the \texttt{t-statistic}
can be expressed as:

\[
    t = \frac{\hat{\beta}}{\text{SE}(\hat{\beta})} = 
    \frac{\sqrt{(n - 1)} \sqrt{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2} \left( \sum_{i = 1}^n x_iy_i \right)}
    {\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2 \right) \sqrt{\sum_{i = 1}^n \left( y_i - x_i\hat{\beta} \right)^2}}.
\] Note that we can do some cancelling with the
\(\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2\) that shows up in both the
numerator and the denominator. Additionally, \(i\) and \(i^{\prime}\)
indices are the same in this case, we can rewrite the
\texttt{t-statistic} as:

\[
    t = \frac{\sqrt{(n - 1)} \sum_{i = 1}^n x_iy_i}
    {\sqrt{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2} \sqrt{\sum_{i^{\prime} = 1}^n \left( y_{i^{\prime}} - x_{i^{\prime}}\hat{\beta} \right)^2}}
\]

Next, for the sum of squared residuals, we have:

\[
    \sum_{i^{\prime} = 1}^n \left( y_{i^{\prime}} - x_{i^{\prime}}\hat{\beta} \right)^2 = 
    \sum_{i^{\prime} = 1}^n \left( y_{i^{\prime}}^2 - 2x_{i^{\prime}}y_{i^{\prime}}\hat{\beta} + x_{i^{\prime}}^2\hat{\beta}^2 \right) = 
    \sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 - 2\hat{\beta}\sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} + \hat{\beta}^2\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2.
\]

By substituting the formula for \(\hat{\beta}\), this equation then
becomes:

\[
    \sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 - 2\frac{\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} \right)^2}{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2}
    + \frac{\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} \right)^2}{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2} = 
    \sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 - \frac{\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} \right)^2}{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2}.
\]

Multiplying this by \(\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2\) gives
us:

\[
   \left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2 \right)\left(\sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 - \frac{\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} \right)^2}{\sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2}\right) =\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2 \right) \left( \sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 \right) - \left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} \right)^2.
\]

This completes the simplification of the denominator, giving us the
desired formula for the \texttt{t-statistic} for \(H_0: \beta = 0\).

\[
    t = \frac{\left( \sqrt{n-1} \right) \sum_{i = 1}^n x_iy_i}
    {\sqrt{\left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}^2 \right) \left( \sum_{i^{\prime} = 1}^n y_{i^{\prime}}^2 \right) - \left( \sum_{i^{\prime} = 1}^n x_{i^{\prime}}y_{i^{\prime}} \right)^2}}
\] By numerically in R, we can confirm that the above equation is true

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum.xy }\OtherTok{=} \FunctionTok{sum}\NormalTok{(x }\SpecialCharTok{*}\NormalTok{ y)}
\NormalTok{sum.x2 }\OtherTok{=} \FunctionTok{sum}\NormalTok{(x}\SpecialCharTok{**}\DecValTok{2}\NormalTok{)}
\NormalTok{sum.y2 }\OtherTok{=} \FunctionTok{sum}\NormalTok{(y}\SpecialCharTok{**}\DecValTok{2}\NormalTok{)}
\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{length}\NormalTok{(x) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}\SpecialCharTok{*}\NormalTok{sum.xy)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(sum.x2}\SpecialCharTok{*}\NormalTok{sum.y2 }\SpecialCharTok{{-}}\NormalTok{ sum.xy}\SpecialCharTok{**}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 18.72593
\end{verbatim}

This value of t equals to the \texttt{t-value} of the fitted model
created in (a)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(lm.fit6)}\SpecialCharTok{$}\NormalTok{coefficients[, }\StringTok{"t value"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 18.72593
\end{verbatim}

\textbf{e. Using the results from (d), argue that the t-statistic for
the regression of \(y\) onto \(x\) is the same as the t-statistic for
the regression of \(x\) onto \(y\).}

The expression of \texttt{t\ statistic} found in (d) is symmetric for
\(x\) and \(y\). Thus, when performing regression of \(x\) onto \(y\),
the variables \(x\) and \(y\) will just replace each other, and the
value is not change. This explains why we have the same
\texttt{t\ statistic} value in (a) and (b).

\textbf{f.~In R, show that when regression is performed with an
intercept, the t-statistic for \(H_0: \beta=0\) is the same for the
regression of \(y\) onto \(x\) as it is for the regression of \(x\) onto
\(y\).}

The regression of \(y\) onto \(x\) with an intercept

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.fit8 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
\FunctionTok{summary}\NormalTok{(lm.fit8)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.8768 -0.6138 -0.1395  0.5394  2.3462 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -0.03769    0.09699  -0.389    0.698    
## x            1.99894    0.10773  18.556   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.9628 on 98 degrees of freedom
## Multiple R-squared:  0.7784, Adjusted R-squared:  0.7762 
## F-statistic: 344.3 on 1 and 98 DF,  p-value: < 2.2e-16
\end{verbatim}

The regression of \(x\) onto \(y\) with an intercept

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.fit9 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ y)}
\FunctionTok{summary}\NormalTok{(lm.fit9)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = x ~ y)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.90848 -0.28101  0.06274  0.24570  0.85736 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  0.03880    0.04266    0.91    0.365    
## y            0.38942    0.02099   18.56   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.4249 on 98 degrees of freedom
## Multiple R-squared:  0.7784, Adjusted R-squared:  0.7762 
## F-statistic: 344.3 on 1 and 98 DF,  p-value: < 2.2e-16
\end{verbatim}

The below results show that \texttt{t\ statistic} values in both
regressions are equal

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(lm.fit8)}\SpecialCharTok{$}\NormalTok{coefficients[, }\StringTok{"t value"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)           x 
##  -0.3886346  18.5555993
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(lm.fit9)}\SpecialCharTok{$}\NormalTok{coefficients[, }\StringTok{"t value"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)           y 
##   0.9095787  18.5555993
\end{verbatim}

\hypertarget{question-3}{%
\section{Question 3}\label{question-3}}

\textbf{In this exercise you will create some simulated data and will
fit simple linear regression models to it. Make sure to use set.seed(1)
prior to starting part (a) to ensure consistent results.}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{a. Using the \texttt{rnorm()} function, create a vector, \(x\),
containing 100 observations drawn from a \(N(0,1)\) distribution. This
represents a feature, \(X\).}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{b. Using the \texttt{rnorm()} function, create a vector, eps,
containing 100 observations drawn from a \(\mathrm{N}(0,0.25)\)
distribution i.e.~a normal distribution with mean zero and variance 0.25
.}

In statistics, the conventional notation for a normal distribution with
mean \(\mu\) and standard deviation \(\sigma\) is
N(\(\mu\),\(\sigma^2\)). While in R, the \texttt{rnorm()} function takes
the arguments n, mean, and sd, where n is the number of observations to
be generated, mean is the mean \(\mu\) for the normal distribution, and
sd is the standard deviation \(\sigma\) for the normal distribution.
Thus, to generate 100 observations from an
N(\(\mu\)=0,\(\sigma^2\)=0.25) distribution, we need to use the syntax
\texttt{rnorm(100,\ mean\ =\ 0,\ sd\ =\ 0.5)}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eps }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{c.~Using \(x\) and \(\epsilon\), generate a vector \(y\)
according to the model} \[
Y=-1+0.5 X+\epsilon
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OtherTok{=} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{x }\SpecialCharTok{+}\NormalTok{ eps}
\end{Highlighting}
\end{Shaded}

\textbf{What is the length of the vector \(y\) ?}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{length}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 100
\end{verbatim}

\textbf{What are the values of \(\beta_{0}\) and \(\beta_{1}\) in this
linear model?}

In this model, \(\beta_{0} = -1\) and \(\beta_{1} = 0.5\).

\textbf{d.~Create a scatterplot displaying the relationship between
\(x\) and \(y\). Comment on what you observe.}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(x, y, }\AttributeTok{main =} \StringTok{"Plot for original data (variance = .25)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Assignment-1_files/figure-latex/unnamed-chunk-23-1.pdf}
While a positive linear correlation between x and y is evident, the
introduction of the error term introduces some variability.

\textbf{e. Fit a least squares linear model to predict \(y\) using
\(x\). Comment on the model obtained. How do \(\hat{\beta}_{0}\) and
\(\hat{\beta}_{1}\) compare to \(\beta_{0}\) and \(\beta_{1}\) ?}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.fit10 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x)}
\FunctionTok{summary}\NormalTok{(lm.fit10)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.93842 -0.30688 -0.06975  0.26970  1.17309 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -1.01885    0.04849 -21.010  < 2e-16 ***
## x            0.49947    0.05386   9.273 4.58e-15 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.4814 on 98 degrees of freedom
## Multiple R-squared:  0.4674, Adjusted R-squared:  0.4619 
## F-statistic: 85.99 on 1 and 98 DF,  p-value: 4.583e-15
\end{verbatim}

The least squares regression model to predict \(y\) using \(x\) has the
form \(\hat{Y}=-1.01885+0.49947 X\), which is quite close to the true
relationship \(Y=-1+0.5X+\epsilon\). The \texttt{p-values} of
essentially zero for \(\beta_0\) and \(\beta_1\) provide strong evidence
to reject the null hypotheses \(H_0: \beta_j=0\) for \(j=1,2\). We also
note, however, that \(R^2=0.4674\), which means that the linear
regression model explains just less than half of the variation in \(Y\)
found in the data. Lastly, the residual standard error is 0.4814.

\textbf{f.~Display the least squares line on the scatterplot obtained in
(d). Draw the population regression line on the plot, in a different
color. Use the legend() command to create an appropriate legend.}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(x, y, }\AttributeTok{main =} \StringTok{"Plot for original data (variance = .25)"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(lm.fit10, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{a =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{b =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{4}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{2.25}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Least squares regression"}\NormalTok{, }\StringTok{"Population regression"}\NormalTok{), }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{), }
       \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Assignment-1_files/figure-latex/unnamed-chunk-25-1.pdf}

\textbf{g. Now fit a polynomial regression model that predicts \(y\)
using \(x\) and \(x^{2}\). Is there evidence that the quadratic term
improves the model fit? Explain your answer.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.fit11 }\OtherTok{=} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(x}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\FunctionTok{summary}\NormalTok{(lm.fit11)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = y ~ x + I(x^2))
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.98252 -0.31270 -0.06441  0.29014  1.13500 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -0.97164    0.05883 -16.517  < 2e-16 ***
## x            0.50858    0.05399   9.420  2.4e-15 ***
## I(x^2)      -0.05946    0.04238  -1.403    0.164    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.479 on 97 degrees of freedom
## Multiple R-squared:  0.4779, Adjusted R-squared:  0.4672 
## F-statistic:  44.4 on 2 and 97 DF,  p-value: 2.038e-14
\end{verbatim}

The polynomial regression model, which predicts \(Y\) using \(X\) and
\(X^2\), offers a marginally improved fit, with a residual standard
error of 0.479 (compared to 0.481 in the linear model) and an \(R^2\)
value of 0.4779 (versus 0.4674). However, the \texttt{p-value} for the
\(X^2\) coefficient stands at 0.164. This indicates that, despite the
slightly better model fit from the quadratic term, there's insufficient
statistical evidence to dismiss the null hypothesis
\(H_0: \beta_2 = 0\). This suggests the improved fit may be due to the
model accommodating the data's inherent noise, represented by
\(\epsilon\), rather than capturing a genuine underlying trend.

\textbf{h. Repeat (a)-(f) after modifying the data generation process in
such a way that there is less noise in the data. The model in (c) should
remain the same. You can do this by decreasing the variance of the
normal distribution used to generate the error term \(\epsilon\) in (b).
Describe your results.}

Repeat (a)-(c) with decreased \(\epsilon\)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{x }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{eps.less.noise }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.25}\NormalTok{)}
\NormalTok{y.less.noise }\OtherTok{=} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{x }\SpecialCharTok{+}\NormalTok{ eps.less.noise}
\end{Highlighting}
\end{Shaded}

Repeat (d) to plot data with less noise

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(x, y.less.noise, }\AttributeTok{main =} \StringTok{"Plot for data with less noise (variance = .0625)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Assignment-1_files/figure-latex/unnamed-chunk-28-1.pdf}
Compared to the plot from (d), there is clearly less noise in the data.
This results in the points being more tightly clustered around the line
\(Y=-1+0.5 X+\epsilon\)

Repeat (e) to fit a linear model y onto x

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.fit.less.noise }\OtherTok{=} \FunctionTok{lm}\NormalTok{(y.less.noise }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
\FunctionTok{summary}\NormalTok{(lm.fit.less.noise)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = y.less.noise ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.46921 -0.15344 -0.03487  0.13485  0.58654 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -1.00942    0.02425  -41.63   <2e-16 ***
## x            0.49973    0.02693   18.56   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.2407 on 98 degrees of freedom
## Multiple R-squared:  0.7784, Adjusted R-squared:  0.7762 
## F-statistic: 344.3 on 1 and 98 DF,  p-value: < 2.2e-16
\end{verbatim}

Repeat (f) to display the new least square line

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(x, y.less.noise, }\AttributeTok{main =} \StringTok{"Plot for data with less noise (variance = .0625)"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(lm.fit.less.noise, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{a =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{b =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{4}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{2.25}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Least squares regression"}\NormalTok{, }\StringTok{"Population regression"}\NormalTok{), }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{), }
       \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Assignment-1_files/figure-latex/unnamed-chunk-30-1.pdf}
Describe the result:

The least squares regression model to predict \(y\) using \(x\) has the
form \(\hat{Y}\)=−1.00942+0.49973X, which is again is quite close to the
true relationship \(Y=−1+0.5X+\epsilon\). The \texttt{p-values} of
essentially zero for \(\beta_0\) and \(\beta_1\) provide strong evidence
to reject the null hypotheses \(H_0:\beta_j=0\) for j=1,2. We also note,
that now \(R^2=0.7784\), which means that the linear regression model
explains about 78\% of the variation in Y found in the less noisy data.
This is a big improvement from the \(R^2\) value of 0.4674 when using
the original, noisier Y. In addition, the residual standard error value
of 0.2407 is also an improvement over the value of 0.4814 from the
original data.

\textbf{i. Repeat (a)-(f) after modifying the data generation process in
such a way that there is more noise in the data. The model in (c) should
remain the same. You can do this by increasing the variance of the
normal distribution used to generate the error term \(\epsilon\) in (b).
Describe your results.}

Repeat (a)-(c) with increased \(\epsilon\)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{x }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{eps.more.noise }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{y.more.noise }\OtherTok{=} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{x }\SpecialCharTok{+}\NormalTok{ eps.more.noise}
\end{Highlighting}
\end{Shaded}

Repeat (d) to plot data with less noise

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(x, y.more.noise, }\AttributeTok{main =} \StringTok{"Plot for data with less noise (variance = 1)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Assignment-1_files/figure-latex/unnamed-chunk-32-1.pdf}
Compared to the plot from (d), there is clearly more noise in the data.
This results in the points being being very spread out from the line
\(Y=-1+0.5 X+\epsilon\)

Repeat (e) to fit a linear model y onto x

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.fit.more.noise }\OtherTok{=} \FunctionTok{lm}\NormalTok{(y.more.noise }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
\FunctionTok{summary}\NormalTok{(lm.fit.more.noise)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = y.more.noise ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.8768 -0.6138 -0.1395  0.5394  2.3462 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -1.03769    0.09699 -10.699  < 2e-16 ***
## x            0.49894    0.10773   4.632 1.12e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.9628 on 98 degrees of freedom
## Multiple R-squared:  0.1796, Adjusted R-squared:  0.1712 
## F-statistic: 21.45 on 1 and 98 DF,  p-value: 1.117e-05
\end{verbatim}

Repeat (f) to display the new least square line

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(x, y.more.noise, }\AttributeTok{main =} \StringTok{"Plot for data with more noise (variance = .0625)"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(lm.fit.more.noise, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{a =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{b =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{4}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{2.25}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Least squares regression"}\NormalTok{, }\StringTok{"Population regression"}\NormalTok{), }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{), }
       \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Assignment-1_files/figure-latex/unnamed-chunk-34-1.pdf}
Describe the result

The least squares regression model to predict \(y\) using \(x\) has the
form \(\hat{Y}\)=−1.03769+0.49894X, which is again is quite close to the
true relationship \(Y=−1+0.5X+\epsilon\). The \texttt{p-values} of
essentially zero for \(\beta_0\) and \(\beta_1\) provide strong evidence
to reject the null hypotheses \(H_0:\beta_j=0\) for j=1,2. We also note,
that now \(R^2=0.1796\), which means that the linear regression model
only explains about 18\% of the variation in Y found in the less noisy
data. This is a big decrease from the \(R^2\) value of 0.4674 when using
the original, which is less noisy. In addition, the residual standard
error value of 0.9628 is very large in comparison with the value of
0.4814 from the original data.

\textbf{j. What are the confidence intervals for \(\beta_{0}\) and
\(\beta_{1}\) based on the original data set, the noisier data set, and
the less noisy data set? Comment on your results.} The confidence
intervals for the original data set:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(lm.fit10)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                  2.5 %     97.5 %
## (Intercept) -1.1150804 -0.9226122
## x            0.3925794  0.6063602
\end{verbatim}

The confidence intervals for the less noisy set:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(lm.fit.less.noise)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                  2.5 %     97.5 %
## (Intercept) -1.0575402 -0.9613061
## x            0.4462897  0.5531801
\end{verbatim}

The confidence intervals for the noisier data set:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(lm.fit.more.noise)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                  2.5 %     97.5 %
## (Intercept) -1.2301607 -0.8452245
## x            0.2851588  0.7127204
\end{verbatim}

As the noise level increases, the confidence intervals for \(\beta_0\)
and \(\beta_1\) become wider and wider.

\hypertarget{question-4}{%
\section{Question 4}\label{question-4}}

\textbf{I collect a set of data (\(n=100\) observations) containing a
single predictor and a quantitative response. I then fit a linear
regression model to the data, as well as a separate cubic regression,
i.e.~\(Y=\beta_{0}+\beta_{1} X+\beta_{2} X_{2}+\beta_{3} X^{3}+\epsilon\)}

\textbf{a. Suppose that the true relationship between \(X\) and \(Y\) is
linear,i.e.~\(Y=\beta_{0}+\beta_{1} X+\epsilon\). Consider the training
residual sum of squares (RSS) for the linear regression, and also the
training RSS for the cubic regression. Would we expect one to be lower
than the other, would we expect them to be the same, or is there not
enough information to tell? Justify your answer.}

\textbf{The polynomial regression is expected to have a lower training
RSS than the linear regression}, because it could make a tighter fit
against data that matched with a wider irreducible error.

\textbf{b. Answer (a) using test rather than training RSS.}

For test RSS, the result is reversed compared to (a). Since the true
relationship is linear, the cubic regression might overfit the training
data, capturing not only the linear trend but also some of the noise.
When applied to a test set, this overfitting might penalize the cubic
model, leading to a higher test RSS compared to the linear model.
\textbf{Therefore, the linear model might have a lower test RSS than the
cubic model in this scenario.}

\textbf{c.~Suppose that the true relationship between \(X\) and \(Y\) is
not linear, but we don't know how far it is from linear. Consider the
training RSS for the linear regression, and also the training RSS for
the cubic regression. Would we expect one to be lower than the other,
would we expect them to be the same, or is there not enough information
to tell? Justify your answer.}

If the true relationship is not linear, the cubic regression, with its
added flexibility, might be better suited to capture the underlying
trend in the training data. \textbf{Therefore, the training RSS for the
polynomial regression model is expected to be lower than that of the
linear regression.}

\textbf{d.~Answer (c) using test rather than training RSS.}

There is not enough information to tell which test RSS would be lower
for either regression, as we do not know ``how far it is from linear''.
If it is closer to linear than cubic, the linear regression test RSS
could be lower than the cubic regression test RSS. Or, if it is closer
to cubic than linear, the cubic regression test RSS could be lower than
the linear regression test RSS. This is due to bias-variance tradeoff:
it is not clear what level of flexibility will fit this data better.

\hypertarget{question-5}{%
\section{Question 5}\label{question-5}}

\textbf{Consider the fitted values that result from performing linear
regression without an intercept. In this setting, the \(i\) th fitted
value takes the form.}

\[
\hat{y}_{i}=x_{i} \hat{\beta}\;(3),
\]

\textbf{where}

\[
\hat{\beta}=\left(\sum_{i=1}^{n} x_{i} y_{i}\right) /\left(\sum_{i^{\prime}=1}^{n} x_{i^{\prime}}^{2}\right)\; (4)
\] \textbf{Show that we can write} \[
\hat{y}_{i}=\left(\sum_{i^{\prime}=1}^{n} a_{i^{\prime}} y_{i^{\prime}}\right)
\] \textbf{What is \(a_{i^{\prime}}\) ?}

\textbf{Note: We interpret this result by saying that the fitted values
from linear regression are linear combinations of the response values.}

By substituting equation (4) into (3), we have: \[
\begin{aligned}
\hat{y}_{i}&=x_{i}\hat{\beta}\\
& = x_i\frac{\sum_{i^{\prime}=1}^{n} x_{i^{\prime}} y_{i^{\prime}}} {\sum_{j=1}^{n} x_{j}^{2}}\\
& = \frac{\sum_{i^{\prime}=1}^{n}x_i x_{i^{\prime}}y_{i^{\prime}}}{\sum_{j=1}^{n}x_{j}^{2}}\\
& = \sum_{i^{\prime}=1}^{n}\frac{x_i x_{i^{\prime}}}{\sum_{j=1}^{n}x_{j}^{2}}y_{i^{\prime}}
\end{aligned}
\] (Note: We've changed the indices used in the formula for
\(\hat{\beta}\) from \(i\) and \(i^{\prime}\) to \(i^{\prime}\) and
\(j\) respectively to distinguish them from the \(i\) in the
\(\hat{y}_i\) formula.)

Comparing with the formula for \(\hat{y}\)
\(\hat{y}_{i}=\left(\sum_{i^{\prime}=1}^{n} a_{i^{\prime}} y_{i^{\prime}}\right)\),
we find the expression for \(a_{i^{\prime}}\) to be: \[
a_{i^{\prime}} = \frac{x_i x_{i^{\prime}}}{\sum_{j=1}^{n}x_{j}^{2}}
\]

\end{document}
